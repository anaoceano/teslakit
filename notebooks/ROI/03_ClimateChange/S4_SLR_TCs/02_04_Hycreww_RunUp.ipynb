{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "... ***CURRENTLY UNDER DEVELOPMENT*** ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyCReeW runup estimation \n",
    "\n",
    "inputs required: \n",
    "  * Nearshore reconstructed historical storms\n",
    "  * Nearshore reconstructed simulated storms\n",
    "  * Historical water levels\n",
    "  * Synthetic water levels \n",
    "  * **Expected Sea Level Rise the Site (3 scenarios)**\n",
    "\n",
    "    \n",
    "in this notebook:\n",
    "  * HyCReWW runup estimation of historical and synthetic events **taking into account the 3 SLR scenarios**\n",
    "  * Extreme value analysis and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# common\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "# pip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# DEV: override installed teslakit\n",
    "import sys\n",
    "sys.path.insert(0, op.join(os.path.abspath(''), '..', '..', '..', '..'))\n",
    "\n",
    "# teslakit\n",
    "from teslakit.database import Database\n",
    "from teslakit.rbf import RBF_Interpolation\n",
    "from teslakit.mda import Normalize\n",
    "\n",
    "from teslakit.plotting.extremes import Plot_ReturnPeriodValidation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Database and Site parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Teslakit database\n",
    "\n",
    "p_data = r'/Users/albacid/Projects/TeslaKit_projects'\n",
    "\n",
    "# offshore\n",
    "db = Database(p_data)\n",
    "db.SetSite('ROI')\n",
    "\n",
    "# climate change - S4\n",
    "db_S4 = Database(p_data)\n",
    "db_S4.SetSite('ROI_CC_S4')\n",
    "\n",
    "\n",
    "# reef characteristics\n",
    "reef_cs = {\n",
    "    'rslope': 0.0505,\n",
    "    'bslope': 0.1667,\n",
    "    'rwidth': 250,\n",
    "    'cf': 0.0105,\n",
    "}\n",
    "\n",
    "\n",
    "# load Hycreww RBF coefficients and sim. variables min. and max.\n",
    "var_lims, rbf_coeffs = db.Load_HYCREWW()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hycreww interpolation \n",
    "\n",
    "def hycreww_runup(var_lims, rbf_coeffs, dset):\n",
    "    '''\n",
    "    Calculates RunUp using hycreww RBFs (level) and linear interpolation (Runup)\n",
    "    \n",
    "    var_lims   - hycreww variables min and max limits\n",
    "    rbf_coeffs - hycreww rbf coefficients\n",
    "    dset       - input dataset (pandas.DataFrame with \"rbf_vns\" columns)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # RBF wave conditions \n",
    "    rbf_hs = [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5]\n",
    "    rbf_hs_lo = [0.005, 0.025, 0.05, 0.005, 0.025, 0.05, 0.005, 0.025, 0.05, 0.005, 0.025, 0.05, 0.005, 0.025, 0.05 ]\n",
    "    rbf_vns = ['level', 'rslope', 'bslope', 'rwidth', 'cf']\n",
    "    \n",
    "    # RBF parameters\n",
    "    ix_sc = [0, 1, 2, 3, 4]\n",
    "    ix_dr = []\n",
    "    minis = [var_lims[x][0] for x in rbf_vns]\n",
    "    maxis = [var_lims[x][1] for x in rbf_vns]\n",
    "    \n",
    "    \n",
    "    # add reef characteristics\n",
    "    for p in reef_cs.keys():\n",
    "        dset[p] = reef_cs[p]\n",
    "\n",
    "    # discard data outside limits\n",
    "    for vn in var_lims.keys():\n",
    "        dset = dset[(dset[vn] > var_lims[vn][0]) &(dset[vn] < var_lims[vn][1])]\n",
    "    \n",
    "    \n",
    "    # RBF dataset to interpolate\n",
    "    ds_in = dset[rbf_vns]\n",
    "\n",
    "    # normalize data\n",
    "    ds_nm ,_ ,_ = Normalize(ds_in.values, ix_sc, ix_dr, minis=minis, maxis=maxis)\n",
    "\n",
    "    # RBF interpolation (with all cases?)\n",
    "    ru_out = []\n",
    "    for rc in rbf_coeffs:\n",
    "        ro = RBF_Interpolation(rc['constant'], rc['coeff'], rc['nodes'], ds_nm.T)\n",
    "        ru_out.append(ro)\n",
    "    ru_z = np.array(ru_out)\n",
    "    \n",
    "    # RU Linear interpolation (hs, hs_lo -> runup)\n",
    "    RU = []\n",
    "    for c, (_, r) in enumerate(dset.iterrows()):\n",
    "        vq = griddata((rbf_hs, rbf_hs_lo), ru_z[:,c], (r['hs'], r['hs_lo2']), method='linear')\n",
    "        RU.append(vq)\n",
    "    RU = np.array(RU)\n",
    "    \n",
    "    # store runup alongside input data\n",
    "    dset_out = dset.copy()\n",
    "    dset_out['runup'] = dset_out['level'] + RU\n",
    "\n",
    "    return dset_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hycreww RBF Interpolation: Historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete historical data and nearshore waves\n",
    "data= db.Load_HIST_Complete_daily() \n",
    "waves = db.Load_NEARSHORE_RECONSTRUCTION_HIST_storms()\n",
    "\n",
    "waves = waves.rename_vars({\"Hs\": \"hs\", \"Tp\": \"tp\", 'Dir':'dir'})  # rename vars\n",
    "waves['hs_lo2'] = waves['hs']/(1.5613*waves['tp']**2)             # calc. hs_lo2\n",
    "waves['level'] = data.sel(time=waves.time).level                  # add level\n",
    "\n",
    "\n",
    "# calculate runup with hycreww\n",
    "dset = waves[['hs', 'tp', 'dir', 'level', 'hs_lo2']].to_dataframe()\n",
    "out_hist = hycreww_runup(var_lims, rbf_coeffs, dset)\n",
    "\n",
    "# store historical runup\n",
    "#db.Save_NEARSHORE_RUNUP_HIST(out_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hycreww RBF Interpolation: Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (scenario: 3, time: 885360)\n",
      "Coordinates:\n",
      "  * time      (time) datetime64[ns] 2000-01-01 ... 2100-12-31T22:58:07.500000256\n",
      "  * scenario  (scenario) float32 0.5 1.0 1.5\n",
      "Data variables:\n",
      "    SLR       (time, scenario) float32 ...\n"
     ]
    }
   ],
   "source": [
    "# Load SLR file\n",
    "\n",
    "SLR = xr.open_dataset('SLR.nc')\n",
    "print(SLR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-572ea0333d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# calculate runup with hycreww\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mww\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hs_lo2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhycreww_runup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_lims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0ml_sims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-72cc1cb33e19>\u001b[0m in \u001b[0;36mhycreww_runup\u001b[0;34m(var_lims, rbf_coeffs, dset)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mRU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mvq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbf_hs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf_hs_lo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mru_z\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hs_lo2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mRU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Bitbucket_repos/teslakit/venv/lib/python3.7/site-packages/scipy/interpolate/ndgriddata.py\u001b[0m in \u001b[0;36mgriddata\u001b[0;34m(points, values, xi, method, fill_value, rescale)\u001b[0m\n\u001b[1;32m    220\u001b[0m         ip = LinearNDInterpolator(points, values, fill_value=fill_value,\n\u001b[1;32m    221\u001b[0m                                   rescale=rescale)\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cubic'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         ip = CloughTocher2DInterpolator(points, values, fill_value=fill_value,\n",
      "\u001b[0;32minterpnd.pyx\u001b[0m in \u001b[0;36mscipy.interpolate.interpnd.NDInterpolatorBase.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Software/Bitbucket_repos/teslakit/venv/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"Convert the input to an array.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load complete simulations data and nearshore waves\n",
    "\n",
    "n_sims_DWT = 10\n",
    "data_list = db.Load_SIM_Complete_storms(n_sims=n_sims_DWT)\n",
    "waves_list = db.Load_NEARSHORE_RECONSTRUCTION_SIM_storms(n_sims=n_sims_DWT)\n",
    "\n",
    "# iterate storms waves simulations\n",
    "l_sims = []\n",
    "for dd, ww in zip(data_list, waves_list):\n",
    "    \n",
    "    ww = ww.rename_vars({\"Hs\": \"hs\", \"Tp\": \"tp\", 'Dir':'dir'})  # rename vars\n",
    "    ww['hs_lo2'] = ww['hs']/(1.5613*ww['tp']**2)                # calc. hs_lo2\n",
    "    ww['level'] = dd.sel(time=ww.time).level                    # add level\n",
    "    \n",
    "    \n",
    "    # TODO: mean water level in the tidal gage in the period 1991-2009 for this base leve\n",
    "    \n",
    "    \n",
    "    # TODO: \n",
    "    # add SLR for the 3 different scenarios. Add the “parabola” since 2000 to 2100\n",
    "    # ww['level'] = ww['level'] + SLR\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate runup with hycreww\n",
    "    dset = ww[['hs', 'tp', 'dir', 'level', 'hs_lo2']].to_dataframe()\n",
    "    out_sim = hycreww_runup(var_lims, rbf_coeffs, dset)\n",
    "    \n",
    "    l_sims.append(out_sim)\n",
    "\n",
    "    \n",
    "# store simulation runup\n",
    "#db.Save_NEARSHORE_RUNUP_SIM(l_sims)\n",
    "# TODO: save new location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Methodology Validation: Annual Maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# compare historical and simulations runup annual maxima\n",
    "hist_A = out_hist.to_xarray()['runup'].groupby('time.year').max(dim='time')\n",
    "sim_A = xr.concat([x.to_xarray()['runup'].groupby('time.year').max(dim='time') for x in l_sims], 'n_sim')\n",
    "\n",
    "# Return Period historical vs. simulations\n",
    "Plot_ReturnPeriodValidation(hist_A, sim_A);\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
